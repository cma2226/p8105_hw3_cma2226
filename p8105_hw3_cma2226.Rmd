---
title: "Data Science Homework 3"
author: "Caroline Andy"
date: "10/10/2020"
output: github_document
---

## Problem 1

First I will load my required packages, including p8105.datasets, which we will use for this problem. I will also set my figure preferences in global options. 

```{r setup, warning = FALSE, message = FALSE}
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = 0.6,
  out.width = "90%"
)
```

Now I will load my data.

```{r load, warning = FALSE, message = FALSE}
data("instacart")
```

Instacart is an online grocery service. In New York City, partner stores include Whole Foods, Fairway, and The Food Emporium. The Instacart Online Grocery Shopping Dataset 2017, used in this question, is an anonymized dataset with over 3 million online grocery orders from more than 200,000 Instacart users. 

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. Observations are at the level of items in orders by users. There are user/order variables -- user ID, order number, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes. 

How many aisles, and which are most items from? 

```{r counting, warning = FALSE, message = FALSE}
instacart %>%
  count(aisle) %>%
  arrange(desc(n))
```

Make a plot showing the number of items ordered in each isle. Only include aisles with +10,000 items purchased. 

```{r plot, warning = FALSE, message = FALSE}
instacart %>%
  count(aisle) %>%
  filter(n > 10000) %>%
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>%
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(
    title = "Frequency of Items Ordered From Each Aisle",
    x = "Aisle",
    y = "Number of Purchased Items"
  )
  
```

Make a table containing three most population items in each of the aisles "baking ingredients," "dog food care," and "packaged vegetable fruits." Include the number of times each item is ordered in your table. 

First let's pull out different aisles, then we will count up most popular items within each aisle.

```{r table}
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>%
  count(product_name) %>%
  mutate(rank = min_rank(desc(n))) %>%
  filter(rank < 4) %>%
  arrange(aisle, rank) %>%
  #clean up table for Rmd
  knitr::kable()
```  

Make a table showing the mean hour of the day which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.

```{r apples_icecream}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```


## Problem 2

I will begin by loading, cleaning and tidying in the required dataset for this problem. This problem uses five weeks of accelerometer data collected on a 63 year-old male with BMI 25, who was admitted to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed with congestive heart failure (CHF). 

```{r load_accel, warning = FALSE, message = FALSE}
accel_data = read_csv("./accel_data.csv") %>%
  #clean variable names
  janitor::clean_names() %>%
  #change table formatting to longer
  pivot_longer(
    cols = starts_with("activity"),
    names_to = "minute", 
    names_prefix = "activity.",
    values_to = "activity") %>%
  #create day_type variable with weekday and weekend entries
  mutate(day_type = ifelse(day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday"), "Weekday", "Weekend")) %>%
  #change variable types as appropriate
  mutate(minute = as.numeric(minute)) %>%
  #reorder variables
  select(day_id, day, week, day_type, minute, activity)
skimr::skim(accel_data)
```

The generated plot contains `r nrow(accel_data)` rows and `r ncol(accel_data)` columns. After cleaning the data, remaining columns include a day identifier, day of the week, week number, day type (weekday vs weekend), minute, and activity level. The "skimr::skim()" code above allows us to assess variable types and summary statistics for each. For the activity variable, the mean activity level is 267 units. The maximum activity level reported at any given minute is 8982 units. 

Traditional analyses of accelerometer data focus on the total activity over the day. I will now aggregate across minutes to create a total activity variable for each day, and create a table showing these totals. 

```{r group_by, warning = FALSE, message = FALSE}
#create a summary dataset containing activity totals for each day and week
summ.accel = accel_data %>%
  group_by(day, week) %>%
  summarise(total = sum(activity))
summ.accel
```

We can investigate trends associated with activity level and day of the week by grouping by day of the week and summarizing. 

```{r summarizing, warning = FALSE, message = FALSE}
summ.day = summ.accel %>%
  group_by(day) %>%
  summarise(total = sum(total)) 
knitr::kable(summ.day)
```

We can investigate trends associated with activity level and week by grouping by week and summarizing. 

```{r summarizing_2, warning = FALSE, message = FALSE}
summ.week = summ.accel %>%
  group_by(week) %>%
  summarise(total = sum(total))
knitr::kable(summ.week)
```

Immediately, we can see that activity levels tend to be highest on Fridays and Wednesdays, and lowest on Saturdays and Tuesdays. We can also see that activity levels were the highest in week 2 and the lowest in week 4. 

We can further visualize these results by generating a single-panel plot that shows the 24-hr activity time courses for each day. I will do so below, using color to indicate day of the week. 

```{r ggplot, warning = FALSE, message = FALSE}
plot = accel_data %>%
  mutate(day = as.factor(day)) %>%
  mutate(day = factor(day, levels = str_c(c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")))) %>%
  ggplot(aes(x = minute, y = activity, color = day)) + 
  geom_line() +
  labs(
    title = "Activity over the course of the day",
      x = "Minute",
      y = "Activity",
      caption = "This graph depicts a patient's daily activity over 35 consecutive days") +
  guides(color = guide_legend("Day"))

plot
```

Based on the above graph, we can make several observations. Firstly, on all days of the week, activity is low during the first ~300 minutes of the day. Similarly, activity declines on all days of the week during the last ~150 minutes of the day. This suggests that the patient is likely sleeping during this timeframe. On Fridays,the patient sustains activity longer into the night than on other weekdays. 

On some days, the patient appears to be more active at different time points. For example, the patient's peak activity occurs on Tuesdays around minute 1150, and on Sundays around minute 850. 


## Problem 3

Next, I will load in the ny_noaa from the p8105.datasets library. 

The National Oceanic and Atmospheric Association (NOAA) of the National Centers for Environmental Information (NCEI) provides public access to some weather data, including the GHCN (Global Historical Climatology Network)-Daily database of summary statistics from weather stations around the world. 

The ny_noaa dataset contains 2,595,176 observations and 7 columns. Noteworthy variables in the ny_noaa dataset include: weather station ID, date, maximum temperature (tenths of degrees C), minimum temperature (tenths of degrees C), precipitation (tenths of mm), snowfall (mm), and snow depth (mm). Each weather station may collect only a subset of these variables, and therefore the resulting dataset contains extensive missing data. 5.6% of the data are missing for the precipitation field; 14.7% of the data are missing for the snow fall field; 27.8% of the data are complete for the snow depth field; 43.7% of the data are complete for the temperature minimum and maximum fields. 

I will begin by cleaning variable names; separating the date variable into three variables denoting year, month, and day; changing variable classes as necessary; and revaluing variable entries such that temperature units are in degrees C and precipitation units are in mm. Snowfall units are kept in mm.   

```{r noaa, warning = FALSE, message = FALSE}
library(p8105.datasets)
data("ny_noaa")

ny_noaa = 
  ny_noaa %>%
  janitor::clean_names() %>%
  separate(date, c("year", "month", "day")) %>%
  mutate(tmax = as.numeric(tmax),
         tmin = as.numeric(tmin),
         year = as.numeric(year),
         month = as.numeric(month),
         day = as.numeric(day)) %>%
  mutate(tmax = (tmax)/10,
         tmin = (tmin)/10,
         prcp = (prcp)/100)
```

For snowfall, what are the most commonly observed values? Why?

```{r count, warning = FALSE, message = FALSE}
ny_noaa %>%
  group_by(snow) %>%
  count(snow, name = "n_obs") %>%
  arrange(desc(n_obs))
```

0 mm is the most commonly observed snowfall value. The most commonly reported snowfall amount greater than 0 is 25 mm. 

Next I will make a two-panel plot showing the average max temperature in January and in July in each station across years.

```{r ggplot_2, warning = FALSE, message = FALSE}
data = filter(ny_noaa, month %in% c("1","7")) %>%
  group_by(year, month, id) %>%
  summarise(mean_tmax = mean(tmax, na.rm = TRUE))

ggplot(data = data, aes(x = year, y = mean_tmax, color = id)) +
  geom_line() +
  facet_grid(~ month) + 
  labs(
    title = "Average max temperatures in January and July",
    x = "Year",
    y = "Maximum temperature in degrees C") +
  theme(legend.position = "none") +
  theme(axis.text.x = element_text(angle = 270, vjust = 0.5, hjust = 1))
  
```

Both graphs have a zig-zag pattern denoting that the average temperature maximum fluctuates between years. Between 1980 and 2010, there appears to be a slight increase in average temperature maximums overtime. The average temperatures in July are more consistent overtime, and have less fluctuation between temperature maximums over the observed years. In the July temperature graph, one weather station appears to have reported an abnormally low high value (< 10 degrees Celsius) during the late 1980s. In the January temperature graph, one weather station appears to have reported an abnormally low high value (-13 degrees Celsius). 

As one would expect, the temperature highs in July are significantly higher than the temperature highs in January. 

Next I will make a two-panel plot showing (i) tmax vs tmin for the full dataset; and (ii) the distribution of snowfall values greater than 0 and less than 100 separately by year. 

```{r ggplot_3, warning = FALSE, message = FALSE}
##generating first plot
hex = ggplot(ny_noaa, aes(x = tmax, y = tmin)) + 
  geom_hex() + 
  labs(
    title = "Density of Reported Temperature Max and Mins",
    x = "Max daily temp (C)",
    y = "Min daily temp (C)")

##generating second plot
snowfall_plot <- ny_noaa %>%
  filter((snow > 0) & (snow < 100)) %>%
  ggplot(aes(x = as.character(year), y = (snow))) +
  geom_boxplot() + 
  labs(
    title = "Distribution of Snowfall Values by Year ",
    x = "Year",
    y = "Snowfall (mm)")
  theme(axis.text.x = element_text(angle = 270, vjust = 0.5, hjust = 1))

##creating a two-panel plot using patchwork 
library(patchwork)               
hex / snowfall_plot

```

The above graphs show several trends with regard to reported temperature maximums and minimums overtime, and the distribution of reported snowfall overtime. The density of reported temperature maximums and minimums plot shows the frequency with which stations have reported given pairs of temperature lows and highs in a given day. The most frequently reported pairs of temperature highs and lows are: i) high temperature of 5 degrees Celsius and low temperature of 0 degrees Celsius, and ii) high temperature of 25 degrees Celsius and low temperature of 15 degrees Celsius. In addition, the diagonal shape of the hex plot suggests that as daily temperature highs increase, temperature lows also increase. 

The snowfall graph shows boxplots of snowfall distrubution by year. Overall, the reported snowfall is fairly consistent overtime, as is revealed through the relatively constant median, and IQRs across boxplots. While the medians have remained consistent overtime at approximately 25 millimeters, the 75th percentile of snowfall was notably less during the years 1998, 2006, and 2010. Several outliers are visible on the high end of snowfall distribution during these same years. 