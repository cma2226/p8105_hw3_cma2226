---
title: "Data Science Homework 3"
author: "Caroline Andy"
date: "10/10/2020"
output: html_document
---

## Problem 1

First I will load my required packages, including p8105.datasets, which we will use for this problem. I will also set my figure preferences in global options. 

```{r setup}
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = 0.6,
  out.width = "90%"
)
```

Now I will load by data

```{r load}
data("instacart")
```

Instacart is an online grocery service. Data description is included in linked webpage about Instacart. {Data dictionary is available.}

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. Observations are at the level of items in orders by users. There are user/order variables -- user ID, order number, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes. {add another sentence or two}. Things to convey have to do with structure. 

How many aisles, and which are most items from? 

```{r counting}
instacart %>%
  count(aisle) %>%
  arrange(desc(n))
```

Make a plot showing the number of items ordered in each isle. Only include aisles with +10,000 items purchased. 

```{r plot}
instacart %>%
  count(aisle) %>%
  filter(n > 10000) %>%
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>%
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

Make a table containing three most population items in each of the aisles "baking ingredients," "dog food care," and "packaged vegetable fruits." Include the number of times each item is ordered in your table. 

First let's pull out different aisles, then we will count up most popular items within each aisle.

```{r table}
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>%
  count(product_name) %>%
  mutate(rank = min_rank(desc(n))) %>%
  filter(rank < 4) %>%
  arrange(aisle, rank) %>%
  #clean up table for Rmd
  knitr::kable()
```  

Make a table showing the mean hour of the day which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.

```{r apples_icecream}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```



## Problem 2

I will begin by loading, cleaning and tidying in the required dataset for this problem. This problem uses five weeks of accelerometer data collected on a 63 year-old male with BMI 25, who was admitted to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed with congestive heart failure (CHF). 

```{r load_accel}
accel_data = read_csv("./accel_data.csv") %>%
  #clean variable names
  janitor::clean_names() %>%
  #change table formatting to longer
  pivot_longer(
    cols = starts_with("activity"),
    names_to = "minute", 
    names_prefix = "activity.",
    values_to = "activity") %>%
  #create day_type variable with weekday and weekend entries
  mutate(day_type = ifelse(day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday"), "Weekday", "Weekend")) %>%
  #change variable types as appropriate
  mutate(minute = as.numeric(minute),
         day_id = as.character(day_id))
```

Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, I will aggregate across minutes to create a total activity variable for each day, and create a table showing these totals. 

```{r group_by}
#create a summary dataset containing activity totals for each day and week
summ.accel = accel_data %>%
  group_by(day, week) %>%
  summarise(total = sum(activity))

#we can investigate trends associated with activity level and day of the week by grouping by day of the week and summarizing
summ.day = summ.accel %>%
  group_by(day) %>%
  summarise(totl = sum(total))

#we can investigate trends associated with activity level and week by grouping by week and summarizing
summ.week = summ.accel %>%
  group_by(week) %>%
  summarise(total = sum(total))
```

Immediately, we can see that activity levels tend to be highest on Fridays and Wednesdays, and lowest on Saturdays and Tuesdays. We can also see that activity levels were the highest in week 2 and the lowest in week 4. 

We can further visualize these results by generating a single-panel plot that shows the 24-hr activity time courses for each day. I will do so below, using color to indicate day of the week. 

```{r ggplot}
plot = accel_data %>%
  group_by(day_id) %>%
  ggplot(aes(x = minute, y = activity, color = day)) + 
  geom_point()

plot
```


Describe in words any patterns or conclusions you can make based on this graph (ie. less activity during nighttime, it does look like weekends are less active than week days, etc.)


## Problem 3

Next, I will load in the ny_noaa from the p8105.datasets library. 

The National Oceanic and Atmospheric Association (NOAA) of the National Centers for Environmental Information (NCEI) provides public access to some weather data, including the GHCN (Global Historical Climatology Network)-Daily database of summary statistics from weather stations around the world. 

The ny_noaa dataset contains 2,595,176 observations and 7 columns. Noteworthy variables in the ny_noaa dataset include: weather station ID, date, maximum temperature (tenths of degrees C), minimum temperature (tenths of degrees C), precipitation (tenths of mm), snowfall (mm), and snow depth (mm). Each weather station may collect only a subset of these variables, and therefore the resulting dataset contains extensive missing data. 5.6% of the data are missing for the precipitation field; 14.7% of the data are missing for the snow fall field; 27.8% of the data are complete for the snow depth field; 43.7% of the data are complete for the temperature minimum and maximum fields. 


I will begin by cleaning variable names; separating the date variable into three variables denoting year, month, and day; changing variable classes as necessary; and revaluing variable entries such that temperature units are in degrees C, precipitation units are in mm, and snowfall units are in cm.    

```{r noaa}
library(p8105.datasets)
data("ny_noaa")

ny_noaa %>%
  janitor::clean_names() %>%
  separate(date, c("year", "month", "day")) %>%
  mutate(tmax = as.numeric(tmax),
         tmin = as.numeric(tmin),
         year = as.numeric(year),
         month = as.numeric(month),
         day = as.numeric(day)) %>%
  mutate(tmax = (tmax)/10,
         tmin = (tmin)/10,
         snow = (snow)/10,
         snwd = (snwd)/10,
         prcp = (prcp)/10) %>%
  skimr::skim()
```

For snowfall, what are the most commonly observed values? Why?

```{r count}
ny_noaa %>%
  count(snow, name = "n_obs") %>%
  arrange(desc(n_obs))
```

Aside from 0 cm, the most commonly reported snowfall amount is 25 cm. 

Next I will make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

```{r ggplot_2}
ny_noaa %>%
  janitor::clean_names() %>%
  separate(date, c("year", "month", "day")) %>%
  mutate(tmax = as.numeric(tmax),
         tmin = as.numeric(tmin),
         year = as.numeric(year),
         month = as.numeric(month),
         day = as.numeric(day)) %>%
  mutate(tmax = (tmax)/10,
         tmin = (tmin)/10,
         snow = (snow)/10,
         snwd = (snwd)/10,
         prcp = (prcp)/10) %>%
  filter(month %in% c("1","7")) %>%
  group_by(year, id, month) %>%
  summarise(mean_tmax = mean(tmax, na.rm = TRUE))

%>%
  ggplot(aes(x = year, y = mean_tmax)) +
  geom_point(alpha = .5) + 
  facet_grid(~month)
```



Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by ye

```{r ggplot_3}
library(patchwork)




